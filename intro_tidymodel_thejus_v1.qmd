---
title: "introduction_to_tidymodels"
format: html
editor: visual
---

## Introduction to Tidymodels

Necessary packages are loaded. We use the `penguins` data as we are acquainted with it in the prior chapters.

```{r}
# Load all the usual suspects with tidymodels
pacman::p_load(conflicted,
               tidyverse,
               palmerpenguins,
               tidymodels)

# Usual workflow - Conflicted packages are taken care of!
conflicts_prefer(dplyr::filter,
                 dplyr::slice,
                 palmerpenguins::penguins)
```

We have seen the tidyverse principles. From a machine learning perspective, the `tidymodels` framework follows `tidyverse` philosophy and essentially is a collection of packages for `modeling` and `machine learning`. It provides a `consistent` and `modular` approach to the entire workflow - from preprocessing to evaluation of model's performance. It is noteworthy to say that `Nextflow`, an NGS workflow tool that you will see later in this course closely resembles tidymodel principles.

*Takeaway* - Forget about different syntaxes for different packages, learn only one `tidymodels` way!

## Step 1: Beginning with tidymodels model!

Let us start by predicting penguin body mass from flipper length using our favorite method `linear regression`.

```{r}
# "Let everyone sweep in front of his own door, and the whole world will be clean"
# Clean the data first - remove the missing na values
penguins_clean <- penguins |>
  drop_na()

# “Choose well. Your choice is brief, and yet endless.”
# Specify the model type and the engine
lm_spec <- linear_reg() |> # Select the linear regression model
  set_engine("lm") # We use R's in-built lm() function

# “By seeking and blundering we learn.”
# Fit the model to data
lm_fit <- lm_spec |>
  fit(body_mass_g ~ flipper_length_mm, data = penguins_clean) # we "assume" as a first step that body_mass_g is a function of flipper length alone.

tidy(lm_fit) # Show me the fitting stats.
```

*Takeaway* - **Specify** the model, then **fit** it.

## Step 2: Making predictions!

So we have trained the model in `Step 1`. Now we `predict`.

```{r}
# Predict on the training data. 
predictions <- lm_fit |>
  predict(new_data = penguins_clean) # Note that here we use the same data we are trained on as the preliminary step.
# This makes no sense now, but wait! We will use another data set when we progress!

# Combine predictions with actual values
results <- penguins_clean |>
  select(body_mass_g, flipper_length_mm) |>
  bind_cols(predictions) # For comparison.

head(results) 
```

Note the `.pred` column. This is the model predictions of `body_mass_g` using flipper length.

*Task* - Imagine what do you expect when we change the predictor from flipper length to bill length?

If it is too hard to imagine, just plug in the `bill_length_mm` to `flipper_length_mm` and see for yourself. Are you getting the same results?

## Step 3: Preprocessing with Recipes

In general, machine learning works better when the data is preprocessed. This is the step when we compare Apples to Apples and **not** Apples to Oranges.

A **recipe** defines the preprocessing steps!

```{r}
# Define a recipe: predict species using all other columns!
penguin_recipe <- recipe(species ~ ., data = penguins_clean) |>
  step_normalize(all_numeric_predictors()) # Center and scale numbers.

penguin_recipe # 
```

Note that `penguin_recipe` doesn't transform the data **yet**. It is just a *recipe*.\
We do the preprocessing steps so that all the columns are scaled in such a way that the mean is 0 and the standard deviation is 1. This is a usual way to take the data away from its units. Technically `step_normalize()` do a `Z-score transformation` of the data.

To actually transform the data, we need two important steps `prep()` and `bake()`.

```{r}
# We prep - Learning the parameters (mean, SD) from the data
prepped <- prep(penguin_recipe) #

# We bake - Apply all the transformations in our pan
baked <- bake(prepped, new_data = NULL) # NULL means to use the training data

head(baked)
#tail(baked)

```

-   `recipe()` - We make a note of all the ingredients

-   `prep`() - Collect and measure the ingredients

-   `bake()` - Cook the data!

    ## Step 4: Defining the model (parsnip)

    Now that we have preprocessed the data with `reicpes`, we will specify the model we want to train. **parsnip** is the module we use for model specification. Note that this is separate from fitting.

    ```{r}
    # Model specification
    model_spec <- multinom_reg() |> # Use multinomal regression
      set_engine("nnet") |> # Using nnet package. Try using "glmnet" too.
      set_mode("classification") # predicting categories instead of numbers which would be regression

    # This creates and object, it won't do anyting unless data is fed in the next steps
    model_spec
    ```

    ## Step 5: Combine into a Workflow

    A **workflow** bags the recipe and model into one object:

    ```{r}
    # Create a workflow
    penguin_wf <- workflow() |> # workflow obj
      add_recipe(penguin_recipe) |> # Add recipe
      add_model(
        multinom_reg() |> # Multinomal regression (for more than 3 categoris)
          set_engine("nnet") |> # use nnet package
          set_mode("classification") # predicting categories instead of numbers
      )
      
    # Fit the entire workflow
    penguin_fit <- penguin_wf |>
      fit(data = penguins_clean)

    penguin_fit
    ```

    **Why use workflows?**

-   Everything is in place in the right order.

-   You can't preprocess test data differently

-   The entire pipeline is one portable object

    ## Step 6: Evaluate Performance

    For every modelling study, there must be some way to check its goodness.\
    We use **yardstick** to find this out.

    ```{r}
    # Get predictions
    preds <- penguin_fit |>
      augment(new_data = penguins_clean) # take the original data and agument it with the predictions

    #preds

    # Calculate the accuracy
    accuracy(preds, truth = species, estimate = .pred_class) # This just gives the fraction of correct predctions from the total predictions.

    # Multiple metrics all at once
    metrics(preds, truth = species, estimate = .pred_class)

    ```

    **Wait... the accuracy is over the charts**. This is too good to be true. Let's investigate why.

    ---

    ## Step 7: Predicting on Training data makes little sense.

    We predicted on the same exact data thate we trained on. Well, there happen to be a term for it **overfitting**. To get a good assessment we split our data:

    ```{r}
    # Split: 75% for training, 25% for testing
    set.seed(100) # So that the random generator works the same way across runtimes.
    data_split <- initial_split(penguins_clean, prop = 0.75, strata = species) # tidy models way of making partitions

    train_data <- training(data_split)
    test_data <- testing(data_split)
    #
    # 

    ```

    Now we refit on the training data and evaluate the test data using the same method as in the previous steps.

    ```{r}
    # Fit on the training data only
    penguin_fit_proper <- penguin_wf |>
      fit(data = train_data)

    # Predict on the test data
    preds_test <- penguin_fit_proper |>
      augment(new_data = test_data)

    # Let us look at the accuracies of both
    comparison <- bind_rows(
      training = accuracy(preds, species, .pred_class),
      testing = accuracy(preds_test, species, .pred_class),
      .id = "data_used"
    )

    comparison
    ```

    We can see that the `testing` accuracy is lower. This is to say that model has a somewhat realistic accuracy of prediction when it sees a new data set.

    *Takeaway* - Evaluation is done on the unseen data!

    ## The Big picture

    **Preprocessing** - `recipe()` -\> `prep()` -\> `bake()`

    **Modelling** - `model_spec()` -\> `set_engine()` -\> `fit()`

    **Workflow** - \``add_recipe()` -\> `add_model()`-\> `fit()`

    **Data split** - `initial_split()` -\> `training()` -\> `testing()`

    **Evaluation** - `augment()` -\> `accuracy()` -\> `metrics()`

-   he tidymodels ecosystem

The core packages bagged in tidymodels are:

-   **parsnip**: Unified interface for model specification

-   **recipes**: Preprocessing and feature engineering

-   **workflows**: Bagging up the preprocessing and modelling steps together

-   **tune**: Hyperparameter optimization

-   **yardstick**: Metrics for model performance/accuracy

-   **rsample**: Data sampling

    The core philosophy lies in the modular approach to modelling wherein each package deals with one part of the modelling process. The packages are designed in a way so that they work together uninterruptadly in cohesive fashion.

    ## Example - Tidymodels and our Penguins

    We will use the penguins dataset here. We will see how the modular approach works in the tidymodels framework with this data.

    ```{r}
    # As always, first step is obviously to clean the data of the spurios na values.
    # We assume that you already have seen this dataset in action.

    penguins_clean <- penguins |>
      drop_na()

    # We will describe what kind of model we want.

    lm_spec <- linear_reg() |> # We choose the linear regression model
      set_engine("lm") # Here we specify the `engine` (You may also choose glmnet)

    # Next step is to fit the model

    lm_fit <- lm_spec |>
      fit(body_mass_g ~ flipper_length_mm, data = penguins_clean)

    # Note that in the above step,  `fit` takes in body_mass_g as a function of flipper_length_mm. Other terms are obvious

    # Next step is to view the results of the linear regression.
    tidy(lm_fit)

    # This is a clean way to see the intercept and slope of the line that is fitted.
    # Other terms include std.error, t-statistic and the p.value

    ```

    It is important to note how tidymodels separates `lm_spec` from `lm_fit` variables.

    ```{r}

    prediction <- lm_fit |>
      predict(new_data = penguins_clean)

    # This may not be the best way as we are essentially doing predictions on the dataset we trained on. But any compatible dataset would do. Consider this just for demonstration. I thought not to complicate with the splitted data, which might be coming in the later chapters.

    results <- penguins_clean |>
      select(body_mass_g, flipper_length_mm) |>
      bind_cols(prediction)

    # Here the predictions are done on the actual values.
    # We select body_mass_g and flipper_length_mm columns
    # After selection bind_cols() binds the two side by side.
    head(results)
    ```

    ## The general philosophy begind tidymodels

    ### Divide and rule!!!

    In tidymodels framework, model **specification** is neatly separated from model **fitt.**

    We see below an example where we specficy a model. In this case we choose the `rand_forest` or the Random Forest Model.\
    **Note** that we are **not fitting** the model yet. This creates a model specification object which can be reused on other datasets. The `engine` argument or the specific package for the fit can be easily replaced by others. This makes the overall workflow easy to manage and clean.

    ```{r}
    rf_spec <- rand_forest(trees = 500) |>
      set_engine("ranger") |> #Note `ranger` can be replaced by say `randomForest`
      set_mode("classification")

    rf_spec # Note that we did not fit anything. Look at the output.
    ```

    So,

-   engines can be replaced at ease, for instance try `randomForest` instead of `ranger`.

-   Model specifications are reusable

-   The workflow is more clean and easily debuggable

    ### Output

    When we talk about consistency in tidymodels, we talk about the tidy data frames. Every function of tidymodels return tidy dataframes. In the following example we will view the model outputs using 3 tidymodel functions, `tidy()`, `glance()` and `augment`.

    ```{r}
    tidy(lm_fit) # Returns a tibble with the model coefficients
    glance(lm_fit) # Statistics R-squared, adjusted r-squared, p_value etc.
    augment(lm_fit, new_data = penguins_clean |>
              head(10))  #The predeiction are added to data, you can add more rows say head(10)
    ```

    ## Pillars of tidyverse

    ### recipes: Preprocessing

    A `recipe` prepares the data before modelling. As with the recipe for a dish to cook, this lists the steps to prepare the data. But it **dont do** anything until **prep** and **bake** it. Rings a bell for all who have ever cooked something in their lifetime.

    ```{r}
    penguin_recipe <- recipe(species ~ ., data = penguins_clean) |>
      step_normalize(all_numeric_predictors()) |> # Put everything on a similar scale
      step_dummy(all_nominal_predictors()) # one-hot method

    penguin_recipe
    ```

    As we noted before, `recipe` doesn't cook or transform the data. It just specifies the ingredients.

    ```{r}
    # First step is to `prep` or prepare the recipe
    prepped <- prep(penguin_recipe) # Don't get alarmed vegans, we aren't harming any penguins here.

    # Next is to `bake` the data
    baked <- bake(prepped, new_data = NULL) # NULL is input as we use the training data

    head(baked)
    ```

    ### Parsnip: Model interface

    Parsnip provides a modelling interface to many engines:

    ```{r}
    # mulitmodal with nnet engine
    multinom_nnet <- multinom_reg() |> 
      set_engine("nnet") # Multinomal logistic regression
    # with glmnet engine
    multinom_glmnet <- multinom_reg() |>
      set_engine("glmnet")

    # Using differnt modes `set_mode`
    # The idea here is that many models can do classification or regression. These are the `modes` in the model

    knn_classi <- nearest_neighbor() |>
      set_mode("classification")

    knn_reg <- nearest_neighbor() |> set_mode("regression")
    ```

    ### workflows: Bundle recipe + model

    A workflow bundles preprocessing and modeling into one unified object:

    ```{r}
    penguin_wf <- workflow() |> # states the workflow pipes to
      add_recipe(penguin_recipe) |> # Recipe
      add_model(multinom_reg() |> # model 
        set_engine("nnet") |>     # sub in model, the engine
        set_mode("classification")) # choose classification or regression

    # Should I explain more? This is very clear in my opinion. Everything was shown before.

    # Workflow fitting
    # Now comes the object to data part

    penguin_fit <- penguin_wf |>
      fit(data = penguins_clean)
    # THe above is the most important step imo, but it needs little explanation as everyting is just `fit`ed together - prep recipe, bake and fit
    penguin_fit
    ```

    ### Yardstick: Model metrics

    After taining, there must be a way to evaluate the accuracy or robustness of the model.\
    Yardsticks provides such tidy metrics functions.

    ```{r}
    # This is simple but hard to understand for me compared to others.
    # I think Andreas will have to give an introduction about this as well, or I will expand this.

    ### augment() function.
    # Takes in new data > Applies recipie transformations > Makes predictions > Returns predictions WITH original data

    preds <- penguin_fit |>
      augment(new_data = penguins_clean) #

    #tail(preds) # Check!

    ### accuracy() function
    # accuracy() - What fraction of the predictions are a match.

    # Args: 
    #     data: Tibble containing truth and predicted values 
    #     truth: Actual column (Here we have `species`)
    #     estimate: Predicted colun (given as .pred_class)

    accuracy(preds, truth = species, estimate = .pred_class)

    ### metrics()
    # In order to view several metrics all at one.

    metrics(preds, truth = species, estimate = .pred_class)

    ```
